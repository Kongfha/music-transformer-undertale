{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "22kCrBW1Osl5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kongfha/music-transformer-undertale/blob/main/Generate_Music_With_Music_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22kCrBW1Osl5"
      },
      "source": [
        "#### Â© Copyright 2021 Aditya Gomatam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIqOYXaUOvNX"
      },
      "source": [
        "This file is part of music-transformer (https://github.com/spectraldoy/music-transformer), my project to build and\n",
        "train a Music Transformer. music-transformer is open-source software licensed under the terms of the GNU General\n",
        "Public License v3.0. music-transformer is free software: you can redistribute it and/or modify it under the terms of\n",
        "the GNU General Public License as published by the Free Software Foundation, either version 3 of the License,\n",
        "or (at your option) any later version. music-transformer is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
        "See the GNU General Public License for more details. A copy of this license can be found within the GitHub repository\n",
        "for music-transformer, or at https://www.gnu.org/licenses/gpl-3.0.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJDpARuOzoD"
      },
      "source": [
        "# Generate Music with Music Transformer!\n",
        "\n",
        "If you're seeing this on the GitHub repository:\n",
        "\n",
        "1. Click the Open in Colab link\n",
        "2. The notebook should be pre-configured with a GPU, but in case it isn't, go to Runtime > Change runtime type, and select GPU for Hardware accelerator\n",
        "3. Run the cells you wish to run with the Play buttons at their top lefts\n",
        "\n",
        "This Notebook lets you play with pretrained [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf) models to generate Western Classical piano music.\n",
        "\n",
        "There are 3 models available on the [GitHub repository](https://github.com/spectraldoy/music-transformer), of which the one trained on pieces by Chopin is automatically loaded, and with which you can immediately generate music. If you wish, you may edit the code under the markdown cells in this notebook to generate music with another of those models, or even upload your own to generate music with. I find that all of my models are highly prone to repeated notes and wandering melodies, but their harmonies are amazingly consistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "croQsX5DO2Oo"
      },
      "source": [
        "#@title Set up Generation\n",
        "#@markdown Clone into the GitHub repository, import required libraries,\n",
        "#@markdown download the Google Magenta SoundFont, and set up variables\n",
        "#@markdown for music generation. You must run this cell if you want to generate any music..\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "from random import randint\n",
        "from IPython.display import Audio\n",
        "\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 ./ \n",
        "\n",
        "!apt install fluidsynth\n",
        "!pip install mido\n",
        "\n",
        "!git clone https://github.com/Kongfha/music-transformer-undertale.git\n",
        "\n",
        "gen_path = \"./gen_audio.mid\"\n",
        "wav_path = \"./gen_audio.wav\"\n",
        "model_path = \"./music-transformer-undertale/ThdNewSave_path.pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8zE1uQbODUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c89c0b-cee5-4eed-af87-58644cff8561"
      },
      "source": [
        "#@title Generate a Piano Performance from Scratch!\n",
        "\n",
        "#@markdown The Music Transformer\n",
        "#@markdown is an autoregressive model, which means it generates outputs one\n",
        "#@markdown at a time, then looks to all its previous outputs in order to\n",
        "#@markdown generate the rest. This process should take about a minute or so on a GPU in the best case.\n",
        "#@markdown Sometimes these models just don't stop generating, which is why you can also KeyboardInterrupt,\n",
        "#@markdown or press the stop button at the top left to interrupt execution and save whatever has already been\n",
        "#@markdown generated.\n",
        "#@markdown\n",
        "#@markdown Additionally, you can set the approximate tempo here, but the actual tempo\n",
        "#@markdown of the output will depend greatly on what notes and what rhythm\n",
        "#@markdown are generated by the model.\n",
        "\n",
        "tempo = 115 #@param {type:'slider', min:32, max:208}\n",
        "\n",
        "#@markdown Note that \n",
        "#@markdown the model cannot generate complete pieces, but only\n",
        "#@markdown abrupt \"sections\" of pieces. This is because only\n",
        "#@markdown about 2000 MIDI events could be input at a time\n",
        "#@markdown during training, whereas most pieces consist of 10 000 - 100 000 MIDI events. \n",
        "\n",
        "choice = randint(0, 1)\n",
        "temp = choice * randint(500, 800) / 1000 + (1 - choice) * randint(1000, 1200) / 1000\n",
        "!python ./music-transformer-undertale/generate.py {model_path} {gen_path} -v -t {temp} -tm {tempo}\n",
        "\n",
        "print(\"Creating playable audio...\")\n",
        "os.system(f\"fluidsynth -ni Yamaha-C5-Salamander-JNv5.1.sf2 {gen_path} -F {wav_path} -r 44100 -g 1.0\")\n",
        "Audio(wav_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/music-transformer-undertale/layers.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  coeffs = 1 / torch.pow(10000, 2 * (k // 2) / d_model)\n",
            "Greedy decoding...\n",
            "Generated 238 tokens. Time taken: 1.47 secs.\n",
            "Saving midi file at ./gen_audio.mid...\n",
            "Done\n",
            "Creating playable audio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LbZZByVQXr0"
      },
      "source": [
        "#@title Download Performance as MIDI file\n",
        "#@markdown You can download the generated .wav file by\n",
        "#@markdown clicking on the 3 dots in the displayed \n",
        "#@markdown Audio. Run this cell to download \n",
        "#@markdown the performance as MIDI.\n",
        "files.download(gen_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}